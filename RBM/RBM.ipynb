{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "question_id": 1
   },
   "source": [
    "# RecSyst  Resticted Boltzmann Machine\n",
    "\n",
    "#### Méthodes utiles pour l'entrainement\n",
    "\n",
    "* sigmoid\n",
    "* update hidden state\n",
    "    * probability and state found\n",
    "    (Only the last update we use probability for avoid biais)\n",
    "* update visible state\n",
    "    * probability and state found\n",
    "    (it's relevant to use probability for reducing noise and thus allowing faster learning\n",
    "* update weight and biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "question_id": 1
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    \n",
    "    \"\"\" Restricted Boltzmann Machine \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden, num_visible, learning_rate = 0.1):\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_visible = num_visible\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \"\"\" Initialize weight matrix (num_visible * num_hidden)  / vbiais and hbiais \"\"\"\n",
    "        \"\"\"Use small random values for the weights chosen from a zero-mean Gaussian \n",
    "        with a standard deviation of 0.01. Set the hidden biases to 0. Set the visible \n",
    "        biases to log[pi/(1−pi)] where pi is the proportion of training vectors in which unit i is on.\"\"\"\n",
    "        \n",
    "        self.weight = 0.01 * np.random.randn(self.num_visible , self.num_hidden)\n",
    "        self.weight = np.insert(self.weight, 0, 0, axis = 0)\n",
    "        self.weight = np.insert(self.weight, 0, 0, axis = 1)\n",
    "    \n",
    "    #function usefull for training\n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def hidden_act(self, vis):\n",
    "        prob_h_act = sigmoid(np.dot(vis,self.weight))\n",
    "        n = vis.shape[0]\n",
    "        \n",
    "        hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "        hidden_states = prob_h_act > np.random.rand(n, self.num_hidden + 1)\n",
    "        # Ignore the bias units.\n",
    "        hidden_states = hidden_states[:,1:]\n",
    "        return [prob_h_act ,hidden_states]\n",
    "    \n",
    "    def visible_act(self, hid):\n",
    "        prob_v_act = sigmoid(np.dot(hid, np.transpose(self.weight)))\n",
    "        n = hid.shape[0]\n",
    "        \n",
    "        visible_states = np.ones((n, self.num_visible + 1))\n",
    "        visible_states[:,:] = prob_v_act > np.random.rand(n, self.num_visible + 1)\n",
    "        # Ignore the bias units.\n",
    "        visible_states = visible_states[:,1:]\n",
    "        \n",
    "        return [prob_v_act , visible_states]\n",
    "    \n",
    "    \"\"\" Update weight and biais \"\"\"\n",
    "    def update_weight(self , pos_step_w , neg_step_w , n):\n",
    "        self.weight += self.learning_rate(pos_step_w - neg_step_w) / n\n",
    "        return (self.weight)\n",
    "         \n",
    "    \n",
    "    \n",
    "    \"\"\" Train modéle positive phase and negative phase \"\"\"  \n",
    "    def train(self, data, max_epoch = 500):\n",
    "    \"\"\" Don't forget to add biais unit \"\"\"\n",
    "        \n",
    "        for epoch in np.arange(max_epoch):\n",
    "            # visible unit to hidden unit\n",
    "            # this is the \"positive phase\" aka the reality phase \n",
    "            prob_h_act, act_h = hidden_act(data)\n",
    "            pos_step_w = np.dot(np.transpose(data),pro_h_act) \n",
    "            \n",
    "            #hidden unit to visible unit\n",
    "            #this is the \"negative phase\" aka the daydreaming phase\n",
    "            prob_v_act, act_v = visible_act(data)\n",
    "            neg_step_w = np.dot(np.transpose(prob_v_act),prob_h_act)\n",
    "            \n",
    "            # update weight\n",
    "            self.weight = update_weight(pos_step_w , neg_step_w , data.shape[0])\n",
    "            \n",
    "            # error \n",
    "            error = np.mean((data - act_v)**2)\n",
    "            \n",
    "            print(\"Epoch %s: error is %s\" % (epoch, error))\n",
    "        \n",
    "        return (prob_v_act , act_v , prob_h_act , act_h)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "question_id": 1
   },
   "outputs": [],
   "source": [
    "def daydream(self, num_samples):\n",
    "        \"\"\"\n",
    "        Randomly initialize the visible units once, and start running alternating Gibbs sampling steps\n",
    "        (where each step consists of updating all the hidden units, and then updating all of the visible units),\n",
    "        taking a sample of the visible units at each step.\n",
    "        Note that we only initialize the network *once*, so these samples are correlated.\n",
    "        Returns\n",
    "        -------\n",
    "        samples: A matrix, where each row is a sample of the visible units produced while the network was\n",
    "        daydreaming.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a matrix, where each row is to be a sample of of the visible units \n",
    "        # (with an extra bias unit), initialized to all ones.\n",
    "        samples = np.ones((num_samples, self.num_visible + 1))\n",
    "\n",
    "        # Take the first sample from a uniform distribution.\n",
    "        samples[0,1:] = np.random.rand(self.num_visible)\n",
    "\n",
    "        # Start the alternating Gibbs sampling.\n",
    "        # Note that we keep the hidden units binary states, but leave the\n",
    "        # visible units as real probabilities. See section 3 of Hinton's\n",
    "        # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "        # for more on why\n",
    "\n",
    "    for i in range(1, num_samples):\n",
    "        visible = samples[i-1,:]\n",
    "\n",
    "        # Calculate the activations of the hidden units.\n",
    "        hidden_activations = np.dot(visible, self.weights)      \n",
    "        # Calculate the probabilities of turning the hidden units on.\n",
    "        hidden_probs = self._logistic(hidden_activations)\n",
    "        # Turn the hidden units on with their specified probabilities.\n",
    "        hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
    "        # Always fix the bias unit to 1.\n",
    "        hidden_states[0] = 1\n",
    "\n",
    "        # Recalculate the probabilities that the visible units are on.\n",
    "        visible_activations = np.dot(hidden_states, self.weights.T)\n",
    "        visible_probs = self._logistic(visible_activations)\n",
    "        visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
    "        samples[i,:] = visible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "question_id": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def hidden_act(hbiais,vis,weight):\n",
    "        prob_h_act = sigmoid(hbiais + np.dot(vis,weight))\n",
    "\n",
    "        act_h = prob_h_act > np.random.uniform(0,1,weight.shape[1])\n",
    "        return [prob_h_act,act_h]\n",
    "    \n",
    "a , b = hidden_act([0.3,0.4], [1,0,1] , np.array([[0.3,0.8],[0.2,0.6],[0.1,0.2]]))\n",
    "[0]*2\n",
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "question_id": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "question_id": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((6, 9 + 1))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "DataScientest - Edit",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "n_questions": 1
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
